{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# /home/jovyan/.imgenv-optimistic-hellman-0/bin/python3 -u /home/jovyan/gpt13/13b_pretrain_119000/train_new.py > /home/jovyan/gpt13/13b_pretrain_119000/train_log.txt 2>&1\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from pynvml import *\n",
    "from transformers import GPT2Tokenizer, GPT2Model, AutoModelForCausalLM, AutoTokenizer\n",
    "from sftDataSet import SFTDataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from transformers import GPT2Tokenizer, GPT2Model, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model, AutoModelForCausalLM, AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'artifacts/cache/'\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    #return trainable_params, all_param, 100 * trainable_params / all_param\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "    \n",
    "\n",
    "res_dir = '/home/aegotovtsev/neural_2024/artifacts/result/'\n",
    "loss_fn = '/home/aegotovtsev/neural_2024/artifacts/result_loss.json'\n",
    "\n",
    "TOKENIZER_PATH = '/home/aegotovtsev/neural_2024/artifacts/model'\n",
    "MODEL_PATH = '/home/aegotovtsev/neural_2024/artifacts/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(TOKENIZER_PATH, device_map = 'auto')\n",
    "if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, load_in_8bit=False, device_map = 'cuda:0',  # cuda:0\n",
    "    low_cpu_mem_usage=True, torch_dtype=torch.float16, offload_state_dict=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "\"\"\"for module in base_model.modules():\n",
    "    if isinstance(module, bnb.nn.Linear8bitLt):\n",
    "        module.state.memory_efficient_backward = True\"\"\"\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "#model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "#model.model.decoder.project_in = lambda x: x.requires_grad_(True)\n",
    "base_model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "base_model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    fan_in_fan_out=True,\n",
    "    r=16, # 8\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"c_attn\", \"c_fc\", \"c_proj\", 'lm_head'],\n",
    ")\n",
    "\n",
    "# lora_checkpoint = '/home/jovyan/gpt13/13b_pretrain_119000/outputs_instr_psih12/checkpoint-38000'\n",
    "\n",
    "model = get_peft_model(base_model, config)\n",
    "\n",
    "# model = PeftModel.from_pretrained(base_model, lora_checkpoint, is_trainable=True, torch_dtype=torch.float16)  # torch_dtype=torch.float16\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "instruct_dataset_train = []\n",
    "# for dataset_fn, diap in train_datasets:\n",
    "#     with open(dataset_fn, 'r') as file:\n",
    "#         instruct_dataset = file.read().strip()\n",
    "\n",
    "#     instruct_dataset = instruct_dataset.split('<|endoftext|>')\n",
    "#     instruct_dataset = [t.strip() + '<|endoftext|>' for t in instruct_dataset if len(t) > 0]\n",
    "    \n",
    "#     if diap:\n",
    "#         instruct_dataset = instruct_dataset[diap[0]: diap[1]]\n",
    "#     instruct_dataset_train += instruct_dataset\n",
    "\n",
    "\n",
    "instruct_dataset_train = SFTDataset(tokenizer = tokenizer, sft_dataset = \"data/dataset.jsonl\")\n",
    "print(len(instruct_dataset_train), 'len(instruct_dataset)')\n",
    "print('non_trainnig_sample_0:\\n' + instruct_dataset_train[0]['text'])\n",
    "print('training_sample_0:\\n' + instruct_dataset_train[0]['response'])\n",
    "print(instruct_dataset_train[0]['text'] + instruct_dataset_train[0]['response'])\n",
    "instruct_dataset_test = []\n",
    "# for dataset_fn, diap in test_datasets:\n",
    "#     with open(dataset_fn, 'r') as file:\n",
    "#         instruct_dataset = file.read().strip()\n",
    "\n",
    "#     instruct_dataset = instruct_dataset.split('<|endoftext|>')\n",
    "#     instruct_dataset = [t.strip() + '<|endoftext|>' for t in instruct_dataset if len(t) > 0]\n",
    "    \n",
    "#     if diap:\n",
    "#         instruct_dataset = instruct_dataset[diap[0]: diap[1]]\n",
    "        \n",
    "#     instruct_dataset_test += instruct_dataset\n",
    "\n",
    "instruct_dataset_test = SFTDataset(tokenizer = tokenizer, sft_dataset = \"data/dataset.jsonl\")\n",
    "print(len(instruct_dataset_test), 'len(instruct_dataset_test)')\n",
    "\n",
    "# instruct_dataset_train = instruct_dataset_train[:10]\n",
    "# instruct_dataset_test = instruct_dataset_test[:10]\n",
    "\n",
    "# ds_train = Dataset.from_dict({\"content\": instruct_dataset_train})\n",
    "# ds_train = ds_train.map(lambda example: tokenizer(example['content']), batched=True)\n",
    "# ds_train = ds_train.filter(lambda x: len(x['input_ids']) < 2048)\n",
    "\n",
    "# ds_test = Dataset.from_dict({\"content\": instruct_dataset_test})\n",
    "# ds_test = ds_test.map(lambda example: tokenizer(example['content']), batched=True)\n",
    "# ds_test = ds_test.filter(lambda x: len(x['input_ids']) < 2048)\n",
    "ds_test = instruct_dataset_test\n",
    "# ds1 = sorted(ds_train, key=lambda x: -len(x['input_ids']))\n",
    "\n",
    "ds1 = instruct_dataset_train\n",
    "#ds1 = sorted(instruct_dataset_train, key = lambda x: -len(x['input_ids']))\n",
    "#print('longest example', len(ds1[0]['input_ids']))\n",
    "\n",
    "# num_tokens = len(tokenizer)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(' || '.join(f'{key} -> {value}' for key, value in tokenizer.special_tokens_map.items()))\n",
    "\n",
    "# #batch_size = 6\n",
    "# #epochs = 3\n",
    "# #acc_steps = 1\n",
    "# #save_steps = 5000\n",
    "# #max_steps = len(ds1)*epochs//batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#         input_ids = [item[\"input_ids\"] for item in batch]\n",
    "#         labels = [item[\"labels\"] for item in batch]\n",
    "#         attention_mask = [item[\"attention_mask\"] for item in batch]\n",
    "\n",
    "#         input_ids = pad_sequence(\n",
    "#             input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "#         )\n",
    "#         labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "#         attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "#         return {\n",
    "#             \"input_ids\": input_ids,\n",
    "#             \"labels\": labels,\n",
    "#             \"attention_mask\": attention_mask,\n",
    "#         }\n",
    "# loader = DataLoader(ds1, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# from itertools import islice\n",
    "\n",
    "# for x in islice(loader, 1): \n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import os\n",
    "res_dir = '/home/aegotovtsev/neural_2024/artifacts/result/'\n",
    "os.path.join(res_dir, \"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# это позволит сохранять только веса LORA\n",
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "\n",
    "class SaveLoRACallback(TrainerCallback):\n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        self.save_lora_weights(kwargs['model'], self.output_dir, state.global_step)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_lora_weights(model, output_dir, step):\n",
    "        lora_dir = os.path.join(output_dir, f\"lora_weights_step_{step}\")\n",
    "        if not os.path.exists(lora_dir):\n",
    "            os.makedirs(lora_dir)\n",
    "        model.save_pretrained(lora_dir)\n",
    "\n",
    "save_lora_callback = SaveLoRACallback(output_dir=os.path.join(res_dir, \"trained\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.12' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# TYT\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 4\n",
    "acc_steps = 4\n",
    "save_steps = 5\n",
    "logging_steps = 5\n",
    "max_steps = len(ds1) * epochs // batch_size\n",
    "print(max_steps)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=ds1, \n",
    "    # eval_dataset=ds_test,\n",
    "    eval_dataset=None,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=acc_steps,\n",
    "        do_train=True,\n",
    "        warmup_steps=250,\n",
    "        max_steps=max_steps, \n",
    "        learning_rate=3e-5,\n",
    "        fp16=True,\n",
    "        logging_steps=logging_steps,\n",
    "        output_dir=res_dir,\n",
    "        logging_strategy='steps',\n",
    "        save_strategy='steps',\n",
    "        save_steps=save_steps,\n",
    "        # do_eval=True,\n",
    "        report_to = [\"tensorboard\"],\n",
    "        # evaluation_strategy='steps',\n",
    "        prediction_loss_only=True,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        # eval_accumulation_steps=acc_steps,\n",
    "        # eval_steps=save_steps, \n",
    "        num_train_epochs=epochs\n",
    "    ),\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=[save_lora_callback]\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "t0 = time.time()\n",
    "trainer.train()\n",
    "t1 = time.time()\n",
    "\n",
    "with open(loss_fn, 'w') as f:\n",
    "    json.dump(trainer.state.log_history, f)\n",
    "\n",
    "print('train finished', t1-t0)\n",
    "\n",
    "# model.save_pretrained(res_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
